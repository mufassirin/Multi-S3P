{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNdZyiJlNHPBRpsk1oynfP8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mufassirin/PSP_ss_predictor/blob/main/ss_exp05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_nX87vMmvbB"
      },
      "outputs": [],
      "source": [
        "#Secondary Structure Predictor - Experiment 05\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "   \n",
        "\n",
        "def get_aa_dict():\n",
        "    resname_to_psp_dict = {}\n",
        "    resname_to_psp_dict['G'] = [1,4,7]\n",
        "    resname_to_psp_dict['A'] = [1,3,7]\n",
        "    resname_to_psp_dict['V'] = [1,7,12]\n",
        "    resname_to_psp_dict['I'] = [1,3,7,12]\n",
        "    resname_to_psp_dict['L'] = [1,5,7,12]\n",
        "    resname_to_psp_dict['S'] = [1,2,5,7]\n",
        "    resname_to_psp_dict['T'] = [1,7,15]\n",
        "    resname_to_psp_dict['D'] = [1,5,7,11]\n",
        "    resname_to_psp_dict['N'] = [1,5,7,14]\n",
        "    resname_to_psp_dict['E'] = [1,6,7,11]\n",
        "    resname_to_psp_dict['Q'] = [1,6,7,14]\n",
        "    resname_to_psp_dict['K'] = [1,5,6,7,10]\n",
        "    resname_to_psp_dict['R'] = [1,5,6,7,13]\n",
        "    resname_to_psp_dict['C'] = [1,7,8]\n",
        "    resname_to_psp_dict['M'] = [1,6,7,9]\n",
        "    resname_to_psp_dict['F'] = [1,5,7,16]\n",
        "    resname_to_psp_dict['Y'] = [1,2,5,7,16]\n",
        "    resname_to_psp_dict['W'] = [1,5,7,18]\n",
        "    resname_to_psp_dict['H'] = [1,5,7,17]\n",
        "    resname_to_psp_dict['P'] = [7,19]\n",
        "    return resname_to_psp_dict\n",
        "\n",
        "def get_pc7_dict():\n",
        "    resname_to_pc7_dict = {'A': [-0.350, -0.680, -0.677, -0.171, -0.170, 0.900, -0.476],\n",
        "                'C': [-0.140, -0.329, -0.359, 0.508, -0.114, -0.652, 0.476],\n",
        "                'D': [-0.213, -0.417, -0.281, -0.767, -0.900, -0.155, -0.635],\n",
        "                'E': [-0.230, -0.241, -0.058, -0.696, -0.868, 0.900, -0.582],\n",
        "                'F': [ 0.363, 0.373, 0.412, 0.646, -0.272, 0.155, 0.318],\n",
        "                'G': [-0.900, -0.900, -0.900, -0.342, -0.179, -0.900, -0.900],\n",
        "                'H': [ 0.384, 0.110, 0.138, -0.271, 0.195, -0.031, -0.106],\n",
        "                'I': [ 0.900, -0.066, -0.009, 0.652, -0.186, 0.155, 0.688],\n",
        "                'K': [-0.088, 0.066, 0.163, -0.889, 0.727, 0.279, -0.265],\n",
        "                'L': [ 0.213, -0.066, -0.009, 0.596, -0.186, 0.714, -0.053],\n",
        "                'M': [ 0.110, 0.066, 0.087, 0.337, -0.262, 0.652, -0.001],\n",
        "                'N': [-0.213, -0.329, -0.243, -0.674, -0.075, -0.403, -0.529],\n",
        "                'P': [ 0.247, -0.900, -0.294, 0.055, -0.010, -0.900, 0.106],\n",
        "                'Q': [-0.230, -0.110, -0.020, -0.464, -0.276, 0.528, -0.371],\n",
        "                'R': [ 0.105, 0.373, 0.466, -0.900, 0.900, 0.528, -0.371],\n",
        "                'S': [-0.337, -0.637, -0.544, -0.364, -0.265, -0.466, -0.212],\n",
        "                'T': [ 0.402, -0.417, -0.321, -0.199, -0.288, -0.403, 0.212],\n",
        "                'V': [ 0.677, -0.285, -0.232, 0.331, -0.191, -0.031, 0.900],\n",
        "                'W': [ 0.479, 0.900, 0.900, 0.900, -0.209, 0.279, 0.529],\n",
        "                'Y': [ 0.363, 0.417, 0.541, 0.188, -0.274, -0.155, 0.476]}\n",
        "    return resname_to_pc7_dict\n",
        "\n",
        "resname_to_psp_dict = get_aa_dict()\n",
        "resname_to_pc7_dict = get_pc7_dict()\n",
        "    \n",
        "def read_pssm(fname,seq):\n",
        "    num_pssm_cols = 44\n",
        "    pssm_col_names = [str(j) for j in range(num_pssm_cols)]\n",
        "    with open(fname,'r') as f:\n",
        "        tmp_pssm = pd.read_csv(f,delim_whitespace=True,names=pssm_col_names).dropna().values[:,2:22].astype(float)\n",
        "    if tmp_pssm.shape[0] != len(seq):\n",
        "        raise ValueError('PSSM file is in wrong format or incorrect!')\n",
        "    return tmp_pssm\n",
        "\n",
        "def read_hhm(fname,seq):\n",
        "    num_hhm_cols = 22\n",
        "    hhm_col_names = [str(j) for j in range(num_hhm_cols)]\n",
        "    with open(fname,'r') as f:\n",
        "        hhm = pd.read_csv(f,delim_whitespace=True,names=hhm_col_names)\n",
        "    pos1 = (hhm['0']=='HMM').idxmax()+3\n",
        "    num_cols = len(hhm.columns)\n",
        "    hhm = hhm[pos1:-1].values[:,:num_hhm_cols].reshape([-1,44])\n",
        "    hhm[hhm=='*']='9999'\n",
        "    if hhm.shape[0] != len(seq):\n",
        "        raise ValueError('HHM file is in wrong format or incorrect!')\n",
        "    return hhm[:,2:-12].astype(float)\n",
        "\n",
        "def read_fasta(fasta_path):\n",
        "    files = []\n",
        "    f = open(fasta_path, 'r')\n",
        "    tmp = []\n",
        "    for i in f.readlines():\n",
        "        line = i.strip()\n",
        "        if line[0] == '>':\n",
        "            tmp.append(line[1:])\n",
        "        else:\n",
        "            tmp.append(line)\n",
        "            files.append(tmp)\n",
        "            tmp = []\n",
        "    f.close()      \n",
        "    return files\n",
        "\n",
        "def get_pssm(file, preparation_config):\n",
        "    \n",
        "    filename = file[0].split('.')[0]\n",
        "    fasta_content = \">\" + filename + '\\n' + file[1]\n",
        "    \n",
        "    fasta_path = os.path.join(preparation_config[\"tmp_files_path\"], filename+'.fasta')\n",
        "    output_path = os.path.join(preparation_config[\"tmp_files_path\"], filename+'.txt')\n",
        "    pssm_path = os.path.join(preparation_config[\"tmp_files_path\"], filename+'.pssm')\n",
        "    \n",
        "    f = open(fasta_path, 'w')\n",
        "    f.writelines(fasta_content)\n",
        "    f.close()     \n",
        "\n",
        "    cmd = preparation_config[\"psiblast_path\"] + \" -num_threads \" + str(preparation_config[\"num_threads\"]) + \" -query \" + \\\n",
        "            fasta_path + \" -db \" + preparation_config[\"uniref90_path\"] + \" -out \" + \\\n",
        "            output_path  + \" -num_iterations 3 -out_ascii_pssm \" + pssm_path\n",
        "            \n",
        "    print (cmd)    \n",
        "    \n",
        "    output = os.popen(cmd).read() \n",
        "    \n",
        "    if os.path.exists(output_path):\n",
        "        os.remove(output_path)\n",
        "\n",
        "def get_hhm(file, preparation_config):\n",
        "    \n",
        "    filename = file[0].split('.')[0]\n",
        "    fasta_content = \">\" + filename + '\\n' + file[1]\n",
        "    \n",
        "    fasta_path = os.path.join(preparation_config[\"tmp_files_path\"], filename+'.fasta')\n",
        "    a3m_path = os.path.join(preparation_config[\"tmp_files_path\"], filename+'.a3m')\n",
        "    hhm_path = os.path.join(preparation_config[\"tmp_files_path\"], filename+'.hhm')\n",
        "    hhr_path = os.path.join(preparation_config[\"tmp_files_path\"], filename+'.hhr')\n",
        "   \n",
        "    if not os.path.exists(fasta_path):\n",
        "        f = open(fasta_path, 'w')\n",
        "        f.writelines(fasta_content)\n",
        "        f.close()     \n",
        "\n",
        "    cmd = preparation_config[\"hhblits_path\"] + \" -i \" + fasta_path + \\\n",
        "            \" -ohhm \" + hhm_path + \" -oa3m \" + a3m_path + \" -d \" + preparation_config[\"uniclust30_path\"] + \\\n",
        "            \" -v 0 -maxres 40000 -cpu \" + str(preparation_config[\"num_threads\"]) + \" -Z 0\"\n",
        "            \n",
        "    print (cmd)    \n",
        "    \n",
        "    output = os.popen(cmd).read() \n",
        "    \n",
        "    if os.path.exists(a3m_path):\n",
        "        os.remove(a3m_path)\n",
        "    if os.path.exists(hhr_path):\n",
        "        os.remove(hhr_path)\n",
        "\n",
        "def make_input(file, preparation_config):\n",
        "    \"\"\"\n",
        "    20pssm + 30hhm + 7pc + 19psp\n",
        "    \"\"\"    \n",
        "    filename = file[0].split('.')[0]\n",
        "    fasta = file[1]   \n",
        "    \n",
        "    seq_len = len(fasta)\n",
        "\n",
        "    pssm_path = os.path.join(preparation_config[\"tmp_files_path\"], filename+'.pssm')\n",
        "    hhm_path = os.path.join(preparation_config[\"tmp_files_path\"], filename+'.hhm')\n",
        "    input_path = os.path.join(preparation_config[\"tmp_files_path\"], filename+'.inputs')\n",
        "    \n",
        "    pssm = read_pssm(pssm_path, fasta)\n",
        "    hhm = read_hhm(hhm_path, fasta)\n",
        "    \n",
        "    pc7 = np.zeros((seq_len, 7))\n",
        "    for i in range(seq_len):\n",
        "        pc7[i] = resname_to_pc7_dict[fasta[i]]\n",
        "    \n",
        "    psp = np.zeros((seq_len, 19))\n",
        "    for i in range(seq_len):\n",
        "        psp19 = resname_to_psp_dict[fasta[i]]\n",
        "        for j in psp19:\n",
        "            psp[i][j-1] = 1\n",
        "    \n",
        "    input_data = np.concatenate((pssm, hhm, pc7, psp),axis=1)\n",
        "    assert input_data.shape == (seq_len,76)\n",
        "    np.savetxt(input_path, input_data, fmt=\"%.4f\")\n",
        "\n",
        "#=============================================================================    \n",
        "\n",
        "def read_inputs(filenames, inputs_files_path):\n",
        "    \"\"\"\n",
        "    20pssm + 30hhm + 7pc + 19psp\n",
        "    \"\"\"\n",
        "    inputs_nopadding = []\n",
        "    max_len = 0\n",
        "    inputs_total_len = 0\n",
        "    for filename in filenames:\n",
        "        inputs_ = np.loadtxt((os.path.join(inputs_files_path, filename + \".inputs\")))\n",
        "        \n",
        "        inputs_total_len += inputs_.shape[0]\n",
        "        if inputs_.shape[0] > max_len:\n",
        "            max_len = inputs_.shape[0]\n",
        "        inputs_nopadding.append(inputs_)\n",
        "    \n",
        "    inputs_padding = np.zeros(shape=(len(filenames), max_len, 76))\n",
        "    inputs_mask_padding = np.ones(shape=(len(filenames), max_len))\n",
        "\n",
        "    for i in range(len(filenames)):\n",
        "        inputs_padding[i,:inputs_nopadding[i].shape[0]] = inputs_nopadding[i]\n",
        "        inputs_mask_padding[i,:inputs_nopadding[i].shape[0]] = 0\n",
        "        \n",
        "    #(hhm - 5000) / 1000\n",
        "    inputs_padding[:,:,20:50] = (inputs_padding[:,:,20:50] - 5000)/1000\n",
        "        \n",
        "    return inputs_padding, inputs_mask_padding, inputs_total_len\n",
        "\n",
        "class InputReader(object):\n",
        "\n",
        "    def __init__(self, data_list, num_batch_size, inputs_files_path):\n",
        "\n",
        "        self.data_list = data_list\n",
        "        self.inputs_files_path = inputs_files_path\n",
        "        self.dataset = tf.data.Dataset.from_tensor_slices(self.data_list).batch(num_batch_size)          \n",
        "        \n",
        "        print (\"Data Size:\", len(self.data_list)) \n",
        "    \n",
        "    def read_file_from_disk(self, filenames_batch):\n",
        "        \n",
        "        filenames_batch = [bytes.decode(i) for i in filenames_batch.numpy()]\n",
        "        inputs_batch, inputs_masks_batch, inputs_total_len = \\\n",
        "            read_inputs(filenames_batch, self.inputs_files_path)\n",
        "        \n",
        "        inputs_batch = tf.convert_to_tensor(inputs_batch, dtype=tf.float32)\n",
        "        inputs_masks_batch= tf.convert_to_tensor(inputs_masks_batch, dtype=tf.float32)\n",
        "        \n",
        "        return filenames_batch, inputs_batch, inputs_masks_batch, inputs_total_len\n",
        "            \n",
        "#=============================================================================    \n",
        "\n",
        "def get_ensemble_ouput(name, predictions, x_mask, total_len):\n",
        "    \n",
        "    if name == \"SS\":\n",
        "        \n",
        "        ss_outputs = []\n",
        "        \n",
        "        ss_prediction = tf.nn.softmax(predictions[0])\n",
        "        for i in predictions[1:]:\n",
        "            ss_prediction += tf.nn.softmax(i)\n",
        "        ss_prediction = tf.nn.softmax(ss_prediction)\n",
        "        \n",
        "        x_mask = x_mask.numpy()\n",
        "        ss_prediction = ss_prediction.numpy()\n",
        "        \n",
        "        max_length = x_mask.shape[1]\n",
        "        for i in range(x_mask.shape[0]):\n",
        "            indiv_length = int(max_length-np.sum(x_mask[i]))\n",
        "            ss_outputs.append(ss_prediction[i][:indiv_length])\n",
        "        \n",
        "        ss_outputs_concat = np.concatenate(ss_outputs, 0)\n",
        "        assert ss_outputs_concat.shape[0] == total_len\n",
        "        \n",
        "        return ss_outputs, ss_outputs_concat\n",
        "    \n",
        "    elif name == \"PhiPsi\":\n",
        "        \n",
        "        phi_predictions = []\n",
        "        psi_predictions = []\n",
        "        phi_outputs = []\n",
        "        psi_outputs = []\n",
        "        for i in predictions:\n",
        "            \n",
        "            # i.shape: batch, seq_len, 4\n",
        "            i = i.numpy()\n",
        "            \n",
        "            phi_prediction = np.zeros((i.shape[0], i.shape[1], 1))\n",
        "            psi_prediction = np.zeros((i.shape[0], i.shape[1], 1))\n",
        "\n",
        "            phi_prediction[:,:,0] = np.rad2deg(np.arctan2(i[:,:,0], i[:,:,1]))\n",
        "            psi_prediction[:,:,0] = np.rad2deg(np.arctan2(i[:,:,2], i[:,:,3]))\n",
        "            \n",
        "            phi_predictions.append(phi_prediction)\n",
        "            psi_predictions.append(psi_prediction)\n",
        "        \n",
        "        phi_predictions = np.concatenate(phi_predictions, -1)\n",
        "        phi_predictions = np.median(phi_predictions, -1)\n",
        "\n",
        "        psi_predictions = np.concatenate(psi_predictions, -1)\n",
        "        psi_predictions = np.median(psi_predictions, -1)\n",
        "        \n",
        "        x_mask = x_mask.numpy()\n",
        "        max_length = x_mask.shape[1]\n",
        "        for i in range(x_mask.shape[0]):\n",
        "            indiv_length = int(max_length-np.sum(x_mask[i]))\n",
        "            phi_outputs.append(phi_predictions[i][:indiv_length])\n",
        "            psi_outputs.append(psi_predictions[i][:indiv_length])\n",
        "        \n",
        "        phi_outputs_concat = np.concatenate(phi_outputs, 0)\n",
        "        psi_outputs_concat = np.concatenate(psi_outputs, 0)\n",
        "        assert phi_outputs_concat.shape[0] == psi_outputs_concat.shape[0] == total_len        \n",
        "        \n",
        "        return phi_outputs, psi_outputs, [phi_outputs_concat, psi_outputs_concat]\n",
        "\n",
        "ss8_str = \"CSTHGIEB\"\n",
        "ss8_dict = {}\n",
        "for k,v in enumerate(ss8_str):\n",
        "    ss8_dict[k] = v\n",
        "\n",
        "ss3_str = \"CHE\"\n",
        "ss3_dict = {}\n",
        "for k,v in enumerate(ss3_str):\n",
        "    ss3_dict[k] = v\n",
        "    \n",
        "def output_results(filenames, ss8_outputs, ss3_outputs, phi_outputs, psi_outputs, preparation_config):\n",
        "    \n",
        "    for filename, ss8_output, ss3_output, phi_output, psi_output in \\\n",
        "        zip(filenames, ss8_outputs, ss3_outputs, phi_outputs, psi_outputs):\n",
        "        \n",
        "        output_path = os.path.join(preparation_config[\"output_path\"], filename+\".opus\")\n",
        "        f = open(output_path, 'w')\n",
        "        f.write(\"#\\tSS3\\tSS8\\tPhi\\tPsi\\tP(3-C)\\tP(3-H)\\tP(3-E)\\tP(8-C)\\tP(8-S)\\t(8-T)\\tP(8-H)\\tP(8-G)\\tP(8-I)\\tP(8-E)\\tP(8-B)\\n\")\n",
        "        \n",
        "        assert ss8_output.shape[0] == ss3_output.shape[0] == phi_output.shape[0] == psi_output.shape[0]\n",
        "\n",
        "        for idx, (ss8, ss3, phi, psi) in \\\n",
        "            enumerate(zip(ss8_output, ss3_output, phi_output, psi_output)):\n",
        "            \n",
        "            ss8_cls = ss8_dict[np.argmax(ss8)]\n",
        "            ss3_cls = ss3_dict[np.argmax(ss3)]\n",
        "            \n",
        "            ss3*=100\n",
        "            ss8*=100\n",
        "            \n",
        "            f.write('%i\\t%s\\t%s'%(idx+1,ss3_cls,ss8_cls))\n",
        "            f.write('\\t%3.2f\\t%3.2f'%(phi, psi))\n",
        "            f.write('\\t%3.2f\\t%3.2f\\t%3.2f'%tuple(ss3))\n",
        "            f.write('\\t%3.2f\\t%3.2f\\t%3.2f\\t%3.2f\\t%3.2f\\t%3.2f\\t%3.2f\\t%3.2f\\n'%tuple(ss8))\n",
        "        \n",
        "        f.close()\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEG1Cgj8pu1k"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CNN(keras.Model):\n",
        "    def __init__(self, num_layers, channels=8):\n",
        "        \n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.cnn_layers = [\n",
        "            [keras.layers.Conv2D(channels, kernel_size=(11,1), padding='SAME'),\n",
        "            keras.layers.Conv2D(channels, kernel_size=(21,1), padding='SAME'),\n",
        "            keras.layers.Conv2D(channels, kernel_size=(31,1), padding='SAME'),\n",
        "            keras.layers.Conv2D(channels, kernel_size=(41,1), padding='SAME'),\n",
        "            keras.layers.Conv2D(channels, kernel_size=(51,1), padding='SAME')]\n",
        "        for _ in range(self.num_layers)]\n",
        "\n",
        "        self.bn_layers = [\n",
        "            keras.layers.BatchNormalization()\n",
        "        for _ in range(self.num_layers)]\n",
        "        \n",
        "    def call(self, x, training):\n",
        "\n",
        "        # x.shape (batch_size, max_seq_length, embeded_size)\n",
        "        x = tf.expand_dims(x, -1)\n",
        "        # x.shape (batch_size, max_seq_length, embeded_size, 1)\n",
        "        \n",
        "        for i in range(self.num_layers):\n",
        "            # x.shape (batch_size, max_seq_length, embeded_size, channels)\n",
        "            x = tf.concat((self.cnn_layers[i][0](x), self.cnn_layers[i][1](x),\n",
        "                            self.cnn_layers[i][2](x), self.cnn_layers[i][3](x),\n",
        "                            self.cnn_layers[i][4](x)), -1)\n",
        "            x = self.bn_layers[i](x, training=training)\n",
        "            x = tf.nn.relu(x)\n",
        "\n",
        "        cnn_output = tf.reduce_mean(x, axis=-1)\n",
        "        \n",
        "        return cnn_output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4hHX04dbeZd",
        "outputId": "5f6a0bef-a81e-4a1d-dbf2-c5fa5c0a4286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec  1 11:06:24 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   28C    P0    44W / 400W |      0MiB / 40536MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml h5py "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdMRvdSlYHfH",
        "outputId": "9fe07096-46ba-4d3d-8d69-3b154dbc0763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (6.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.6)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "print(tf.version.VERSION)"
      ],
      "metadata": {
        "id": "DaU5c0EMYOJ-",
        "outputId": "f77a07e9-2e1c-4455-dc13-a94f416a59e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XcjnnWTDYQYo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}